---
title: Faster generalised linear models in largeish data
date: '2018-03-05'
linkTitle: http://notstatschat.tumblr.com/post/171570186286
source: Biased and Inefficient
description: <p>There basically isn’t an algorithm for generalised linear models that
  computes the maximum likelihood estimator in a single pass over the $N$ observatons
  in the data. You need to iterate.  The <b>bigglm</b> function in the <b>biglm</b> package
  does the iteration using bounded memory, by reading in the data in chunks, and starting
  again at the beginning for each iteration. That works, but it can be slow, especially
  if the database server doesn’t communicate that fast with your R process.</p><p
---
